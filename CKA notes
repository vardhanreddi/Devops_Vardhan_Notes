ETCD:
####
etcd is a distributed, reliable key value store which is simple, secure and fast ex: put Name "John Doe" Get Name "John Doe"
WHen you run etcd  ./etcd it starts a service that listens on port 2379. A default client that comes with etcd is etcdctl client. you can use this to store and retrieve key value paris. ex: ./etcdctl set key1 value1  to retrieve: ./etcdctl get key1
Etcd stores the information regarding the cluster such as Node/Pods/Configd/secrets/roles/bindings etc. All the changes made in all objects in the cluster are updated in the etcd server. 
To list all keys stored by k8s run below
$ kubectl exec etcd-master -n kube-system etcdctl get / --prefix -keys-only

ETCD - Commands - (Optional) Additional information about ETCDCTL Utility
##########################################################################

ETCDCTL is the CLI tool used to interact with ETCD.

ETCDCTL can interact with ETCD Server using 2 API versions - Version 2 and Version 3.  By default its set to use Version 2. Each version has different sets of commands.

For example ETCDCTL version 2 supports the following commands:

etcdctl backup
etcdctl cluster-health
etcdctl mk
etcdctl mkdir
etcdctl set

Whereas the commands are different in version 3

etcdctl snapshot save 
etcdctl endpoint health
etcdctl get
etcdctl put

To set the right version of API set the environment variable ETCDCTL_API command
export ETCDCTL_API=3
When API version is not set, it is assumed to be set to version 2. And version 3 commands listed above don't work. When API version is set to version 3, version 2 commands listed above don't work.

Apart from that, you must also specify path to certificate files so that ETCDCTL can authenticate to the ETCD API Server. The certificate files are available in the etcd-master at the following path. We discuss more about certificates in the security section of this course. So don't worry if this looks complex:

--cacert /etc/kubernetes/pki/etcd/ca.crt     
--cert /etc/kubernetes/pki/etcd/server.crt     
--key /etc/kubernetes/pki/etcd/server.key

kubectl exec etcd-master -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt  --key /etc/kubernetes/pki/etcd/server.key" 


Kube Controller Manager
#######################
KCM Manages various controllers in kubernetes. controller is a process that monitors the state of various components in the system and works towards bringing the system to the desired state
1. Node COntroller
NC is responsible for monitoring the state of the nodes and take necessary actions if required
node monitor period=5s -> NC checks the status of the nodes every 5 sec. It does this through the kube-api-server only
Node monitor grace period=40s  -> marks node as "unreachable" if it doesn't get any heartbeats of the nodes. But it waits for 40sec (grace period) before it marks as unreachable
pod eviction timeout=5m -> waits for the node to come back up online within 5 min. If not the NC will remove the pods from that node and provisions them in a healthy node

2. Replication controller
RC is responsible for monitoring state of replica set and maintains the desired number of pods are available at all times

Kube Scheduler
#############
when a pod is created KS only decides which pod goes on which node. The pod is placed on the decided node by the Kubelet
Kube scheduler assigns the pods to the nodes based on 2 criteria 
Filter nodes - filters the nodes and marks the nodes which are not suitable for selection (say nodes with less than 6cpu left) - This is based on the pod capacity (say ex: Pod with 6 cpu required to be assigned to the node)
Rank nodes - the node with best rank gets the pod assigned. ex: node having more cpu to accomodate the pod


Kube-proxy
#########
all pods can communicate with each other using a pod network. like flannel, weavenet etc
any service (ex: clusterIP) we create should be accessible across all nodes in the cluster and this is achieved by kube-proxy. 
KP runs on each node in cluster and everytime it finds a new service created it creates appropriate rules (Ip table rules ) on the nodes and forwards the traffic to those services

Kubelet 
#######

Kubelet doesn't get installed automatically deployed when you deploy the cluster using KUBEADM. we need to manually install kubelt on the worker nodes












